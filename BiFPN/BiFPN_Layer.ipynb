{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiFPN_Layer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4vHlPs8Ydu0"
      },
      "source": [
        "####"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRZojwLEYhTH"
      },
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HfBA7tlYqQs"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdUQn7oxZDyN"
      },
      "source": [
        "def Reverse(lst):\n",
        "    return [ele for ele in reversed(lst)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKK-hp3sZPoN"
      },
      "source": [
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels ,\n",
        "                 kernel_size = (3 , 3) , \n",
        "                 stride = (1 , 1) , \n",
        "                 padding = 1 , \n",
        "                 use_norm = True , \n",
        "                 use_activation = True , \n",
        "                 use_pool = False):\n",
        "        super(Conv , self).__init__()\n",
        "\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "        self.use_pool = use_pool\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels , \n",
        "                               out_channels , \n",
        "                               kernel_size , \n",
        "                               stride , \n",
        "                               padding)\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.InstanceNorm2d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.LeakyReLU(0.2)\n",
        "        if self.use_pool:\n",
        "            self.max_pool = nn.MaxPool2d(kernel_size=2 , stride=2)\n",
        "        \n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_pool:\n",
        "            x = self.max_pool(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhDA1b3fZRsV"
      },
      "source": [
        "\n",
        "class ConvT(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 kernel_size = (2 , 2) , \n",
        "                 stride = (2 , 2) , \n",
        "                 padding = 0 , \n",
        "                 use_norm = True , \n",
        "                 use_activation = True):\n",
        "        super(ConvT , self).__init__()\n",
        "\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "\n",
        "        self.convT = nn.ConvTranspose2d(in_channels , \n",
        "                                        out_channels , \n",
        "                                        kernel_size , \n",
        "                                        stride ,\n",
        "                                        padding)\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.InstanceNorm2d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.convT(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72qMbCqbZUud"
      },
      "source": [
        "class Resnet_Block(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 downsample = False):\n",
        "        super(Resnet_Block , self).__init__()\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "        if self.downsample:\n",
        "            self.conv1 = Conv(in_channels , \n",
        "                        in_channels , \n",
        "                        kernel_size=(2 , 2) , \n",
        "                        stride=(2 , 2) ,\n",
        "                        padding = 0)\n",
        "            \n",
        "            self.conv_skip = Conv(in_channels ,\n",
        "                            out_channels ,\n",
        "                            kernel_size = (2 ,2) , \n",
        "                            stride = (2 , 2) , \n",
        "                            padding = 0)\n",
        "        else:    \n",
        "            self.conv1 = Conv(in_channels , \n",
        "                            in_channels , \n",
        "                            kernel_size=(1 , 1) , \n",
        "                            stride=(1 , 1) ,\n",
        "                            padding = 0)\n",
        "            \n",
        "            self.conv_skip = Conv(in_channels ,\n",
        "                              out_channels ,\n",
        "                              kernel_size = (1 , 1) , \n",
        "                              stride = (1 ,1) , \n",
        "                              padding = 0)\n",
        "            \n",
        "        self.conv2 = Conv(in_channels , \n",
        "                          in_channels)\n",
        "        \n",
        "        self.conv3 = Conv(in_channels , \n",
        "                          out_channels , \n",
        "                          kernel_size = (1 , 1) , \n",
        "                          stride = (1 , 1) , \n",
        "                          padding = 0)\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self , x): \n",
        "        x_ = x.clone()\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x_ = self.conv_skip(x_)\n",
        "        x += x_\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khtTBegKZX9H"
      },
      "source": [
        "\n",
        "class Resnet(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels):\n",
        "        super(Resnet , self).__init__()\n",
        "\n",
        "        self.conv1 = Conv(in_channels , 128 , kernel_size=(7 , 7) , stride=(2 , 2) , padding=3)\n",
        "\n",
        "        self.conv2 = self._make_repeated_blocks(128 , 256 , 3 , downsample = False)\n",
        "        self.conv3 = self._make_repeated_blocks(256 , 512 , 8)\n",
        "        self.conv4 = self._make_repeated_blocks(512 , 1024 , 36)\n",
        "        self.conv5 = self._make_repeated_blocks(1024 , 2048 , 3)\n",
        "        #self.linear = Linear(2048 , out_channels)\n",
        "\n",
        "    def _make_repeated_blocks(self , in_channels , out_channels , repeats , downsample = True):\n",
        "        layers = []\n",
        "        for i in range(repeats):\n",
        "            if i == 0 and downsample == True:\n",
        "                layers.append(Resnet_Block(in_channels , out_channels , downsample=downsample))\n",
        "            elif i == 0:\n",
        "                layers.append(Resnet_Block(in_channels , out_channels))\n",
        "            else:\n",
        "                layers.append(Resnet_Block(out_channels , out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x_0 = self.conv1(x)\n",
        "        x_0_ = torch.max_pool2d(x_0 , kernel_size = (2 , 2) , stride = (2 , 2))\n",
        "        x_1 = self.conv2(x_0_)\n",
        "        x_2 = self.conv3(x_1)\n",
        "        x_3 = self.conv4(x_2)\n",
        "        x_4 = self.conv5(x_3)\n",
        "        #x_out = [x_0 , x_1 , x_2 , x_3]\n",
        "        x_out = [x_4 , x_3 , x_2 , x_1 , x_0]\n",
        "        return x_out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79EsKj0YZh7c"
      },
      "source": [
        "resnet = Resnet(3 , 1000).to(device)\n",
        "x = torch.randn(2 , 3 , 224 , 224).to(device)\n",
        "z = resnet(x)\n",
        "print(z[0].shape , z[1].shape , z[2].shape , z[3].shape , z[4].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDVWKHY5DPNw"
      },
      "source": [
        "class BiFPN_Layer(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels_list = [2048 , 1024 , 512 , 256 , 128] , \n",
        "                 out_channels = [128 , 256 , 512 , 1024 , 2048]):\n",
        "        super(BiFPN_Layer , self).__init__()\n",
        "\n",
        "        self.top_down = nn.ModuleList()\n",
        "        self.bottom_up = nn.ModuleList()\n",
        "\n",
        "        j = 0\n",
        "        for i , channels in enumerate(in_channels_list):\n",
        "            if i == 0 or i == len(in_channels_list)-1:\n",
        "                if i == 0:\n",
        "                    self.top_down.append(ConvT(channels , in_channels_list[i+1]))\n",
        "            else :\n",
        "                self.top_down.append(ConvT(channels * 2 , in_channels_list[i+1]))\n",
        "        \n",
        "        reversed_in_channels_list = Reverse(in_channels_list)\n",
        "\n",
        "        for i , channels in enumerate(reversed_in_channels_list):\n",
        "            if i == 0 or i == len(reversed_in_channels_list) -1 :\n",
        "                if i == 0:\n",
        "                    self.bottom_up.append(Conv(channels*2 , out_channels[i] , stride=(2 , 2)))\n",
        "                else :\n",
        "                    self.bottom_up.append(Conv(channels + out_channels[i-1] , out_channels[i] , stride=(2 , 2)))\n",
        "            else :\n",
        "                self.bottom_up.append(Conv(channels * 2 + out_channels[i-1] , out_channels[i] , stride=(2 , 2)))\n",
        "    \n",
        "    def forward(self , x):\n",
        "        x_1 = []\n",
        "        x_out = []\n",
        "        j = 0\n",
        "        for i , x_ in enumerate(x):\n",
        "          \n",
        "            if i == 0 or i == len(x)-1:\n",
        "                if i == 0:\n",
        "                    x_1.append(self.top_down[j](x_))\n",
        "                    j += 1\n",
        "                elif i == len(x) - 1:\n",
        "                    lamp = 0\n",
        "            else :\n",
        "                temp = torch.cat([x[i] , x_1[-1]] , dim=1)\n",
        "                x_1.append(self.top_down[j](temp))\n",
        "                j+=1\n",
        "        x_1_reversed = Reverse(x_1)\n",
        "        x_reversed = Reverse(x)\n",
        "        j = 0\n",
        "        for i in range(len(x)):\n",
        "            if i == 0 or i == len(x) - 1:\n",
        "                if i == 0:\n",
        "                    temp = torch.cat([x_reversed[i] , x_1_reversed[i]] , dim=1)\n",
        "                    x_out.append(self.bottom_up[j](temp))\n",
        "                    j += 1\n",
        "                else :\n",
        "                    temp = torch.cat([x_reversed[-1] , x_out[-1]] , dim=1)\n",
        "                    x_out.append(self.bottom_up[j](temp))\n",
        "            else :\n",
        "                temp = torch.cat([x_reversed[i] , x_1_reversed[i] , x_out[-1]] , dim=1)\n",
        "                x_out.append(self.bottom_up[j](temp))\n",
        "                j += 1\n",
        "        return x_out"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGq3Fs0bEUWR"
      },
      "source": [
        "bifpn = BiFPN_Layer().to(device)\n",
        "y = bifpn(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrpxMDEtMLML"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels):\n",
        "        super(Model , self).__init__()\n",
        "\n",
        "        self.resnet = Resnet(in_channels , 1000)\n",
        "        self.bifpn = BiFPN_Layer()\n",
        "        self.bifpn1 = BiFPN_Layer(in_channels_list=[2048 , 1024 , 512 , 256 , 128] , \n",
        "                                  out_channels=[128 , 128 , 128 , 128 , 128])\n",
        "\n",
        "    def forward(self  ,x):\n",
        "        x = self.resnet(x)\n",
        "        x = self.bifpn(x)\n",
        "        x = Reverse(x)\n",
        "        x = self.bifpn1(x)\n",
        "        return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39PG-v5EbrsY"
      },
      "source": [
        "model = Model(3).to(device)\n",
        "x = torch.randn(2 , 3 , 448 , 448).to(device)\n",
        "z = model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpQ06rO0b0z4",
        "outputId": "49567a28-398e-4b5e-d0cb-7c794234f3ef"
      },
      "source": [
        "for z_ in z:\n",
        "    print(z_.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 128, 56, 56])\n",
            "torch.Size([2, 128, 28, 28])\n",
            "torch.Size([2, 128, 14, 14])\n",
            "torch.Size([2, 128, 7, 7])\n",
            "torch.Size([2, 128, 4, 4])\n"
          ]
        }
      ]
    }
  ]
}