{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FCN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTIdZ8MnrB4x"
      },
      "source": [
        "####"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjz0bkS8rWQd"
      },
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX4WNdgIgAjj"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWI2UG0rfMt-"
      },
      "source": [
        "class Conv(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 kernel_size = (3 , 3) , \n",
        "                 stride = (1 , 1) ,\n",
        "                 padding = 1 , \n",
        "                 use_norm = True , \n",
        "                 use_activation = True , \n",
        "                 use_pool = False):\n",
        "        super(Conv , self).__init__()\n",
        "\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "        self.use_pool = use_pool\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels , \n",
        "                               out_channels , \n",
        "                               kernel_size , \n",
        "                               stride , \n",
        "                               padding)\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.InstanceNorm2d(out_channels)\n",
        "        if self.use_pool:\n",
        "            self.max_pool = nn.MaxPool2d(kernel_size=2 , stride=2)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_pool:\n",
        "            x = self.max_pool(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        return x        "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te_OtHV3f9_b"
      },
      "source": [
        "x = torch.randn(2 , 3 , 512 , 512).to(device)\n",
        "conv = Conv(3 , 32 , use_pool=True).to(device)\n",
        "z = conv(x)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUvJkZUEgJv3"
      },
      "source": [
        "class ConvT(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels, \n",
        "                 kernel_size = (2 , 2) , \n",
        "                 stride = (2 , 2) , \n",
        "                 padding = 0 , \n",
        "                 use_norm = True , \n",
        "                 use_activation = True):\n",
        "        super(ConvT , self).__init__()\n",
        "\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "\n",
        "        self.convT = nn.ConvTranspose2d(in_channels ,\n",
        "                                         out_channels , \n",
        "                                        kernel_size , \n",
        "                                        stride , \n",
        "                                        padding)\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.InstanceNorm2d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.convT(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6ZF1EiBg46Q"
      },
      "source": [
        "x = torch.randn(2 , 3 , 512 , 512).to(device)\n",
        "conv = ConvT(3 , 32).to(device)\n",
        "z = conv(x)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCuB0Nxvpusb"
      },
      "source": [
        "class FCN_Block(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 use_pool = True):\n",
        "        super(FCN_Block , self).__init__()\n",
        "\n",
        "        self.conv1 = Conv(in_channels , out_channels)\n",
        "        self.conv2 = Conv(out_channels , out_channels , use_pool=use_pool)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEG5RLdh3IeQ"
      },
      "source": [
        "config = [\n",
        "          # [out_channels , use_pool]\n",
        "          [64 , True] ,\n",
        "          [128 , True] , \n",
        "          [256 , False] , \n",
        "          [256 , True] , \n",
        "          'S' , \n",
        "          [512 , False] , \n",
        "          [512 , True] , \n",
        "          'S' , \n",
        "          [512 , False] , \n",
        "          [512 , True] , \n",
        "          [4096 , True] , \n",
        "          (512 , 'C') , \n",
        "          (256 , 'C') , \n",
        "          128 , \n",
        "          64 , \n",
        "          3\n",
        "]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1whD96cl8_oY"
      },
      "source": [
        "class Save(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Save , self).__init__()\n",
        "        self.s = 2\n",
        "    def forward(self , x):\n",
        "        return x"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ffjw62Ng9iW"
      },
      "source": [
        "class FCN(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels = 3 , \n",
        "                 out_channels = 3 , \n",
        "                 config = config):\n",
        "        super(FCN , self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for layer in config:\n",
        "            if isinstance(layer , list):\n",
        "                out_channels , use_pool = layer\n",
        "                self.layers.append(FCN_Block(in_channels , out_channels , use_pool=use_pool))\n",
        "                in_channels = out_channels\n",
        "            elif isinstance(layer , tuple):\n",
        "                out_channels , _ = layer\n",
        "                self.layers.append(ConvT(in_channels , out_channels))\n",
        "                in_channels = out_channels\n",
        "            elif isinstance(layer , str):\n",
        "                self.layers.append(Save())\n",
        "            elif isinstance(layer , int):\n",
        "                out_channels = layer\n",
        "                self.layers.append(ConvT(in_channels , out_channels))\n",
        "                in_channels = out_channels\n",
        "\n",
        "    def forward(self , x):\n",
        "        saved = []\n",
        "        for layer , i in enumerate(self.layers):\n",
        "            if isinstance(layer , FCN_Block):\n",
        "                x = layer(x)\n",
        "            elif isinstance(layer , Save):\n",
        "                x = layer(x)\n",
        "                saved.append(x)\n",
        "            elif isinstance(layer , ConvT):\n",
        "                x = layer(x)\n",
        "                if i == 12 or i == 13:\n",
        "                    x += saved.pop()\n",
        "        return x"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh70k37B-_D8"
      },
      "source": [
        "def test():\n",
        "    x = torch.randn(2 , 3 , 224 , 224).to(device)\n",
        "    fcn = FCN().to(device)\n",
        "    z = fcn(x)\n",
        "    print(z.shape)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31eIR8YQ_MRV"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Leawi_wZ_NF1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}